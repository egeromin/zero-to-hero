Before fixing initialisations
Step 196000: validation loss = 2.2601, validation accuracy = 32.07%
Final test loss = 2.2696, test accuracy = 31.82%

After adding embedding into regularisation
Step 200000: validation loss = 2.4116, validation accuracy = 29.29%
Final test loss = 2.4192, test accuracy = 28.95%
WORSE, REJECTED.

After adding more hidden layers
Step 200000: validation loss = 2.2996, validation accuracy = 31.19%
Final test loss = 2.3032, test accuracy = 31.49%
ACCEPTED FOR NOW DESPITE REGRESSION, for educational purposes
of having multiple hidden layers. Will revert again later.

After fixing initialisation
Step 200000: validation loss = 2.2953, validation accuracy = 31.39%
Final test loss = 2.3021, test accuracy = 31.47%

BatchNorm, but without shift or scale:
performance does not improve beyond a certain point, see logs:
Step 0: validation loss = 3.9930, validation accuracy = 5.25%
Step 4000: validation loss = 2.5880, validation accuracy = 25.64%
Step 8000: validation loss = 2.6173, validation accuracy = 23.56%
Step 12000: validation loss = 2.5988, validation accuracy = 24.14%
Step 16000: validation loss = 2.6116, validation accuracy = 21.14%
Step 20000: validation loss = 2.5936, validation accuracy = 22.89%
Step 24000: validation loss = 2.6023, validation accuracy = 22.26%
Step 28000: validation loss = 2.6125, validation accuracy = 21.95%

Still without shift or scale, but after fixing inference normalisation,
by using running mean and std:
Step 0: validation loss = 3.9929, validation accuracy = 5.25%
Step 4000: validation loss = 2.4873, validation accuracy = 26.80%
Step 8000: validation loss = 2.3943, validation accuracy = 28.69%
Step 12000: validation loss = 2.3783, validation accuracy = 28.62%
Step 16000: validation loss = 2.3620, validation accuracy = 28.41%
Step 20000: validation loss = 2.3600, validation accuracy = 28.52%
Step 24000: validation loss = 2.3540, validation accuracy = 28.26%
Step 28000: validation loss = 2.3586, validation accuracy = 29.10%
Step 32000: validation loss = 2.3392, validation accuracy = 28.91%

With shift and scale, and no kaiming, 32K
Step 32000: validation loss = 2.3414, validation accuracy = 29.45%
Final test loss = 2.3397, test accuracy = 29.94%

Shift and scale, no kaiming, lower learning rate, 32K
Step 32000: validation loss = 2.3359, validation accuracy = 30.03%
Final test loss = 2.3367, test accuracy = 30.17%

Bring kaiming back, even lower learning rate, 32K
Step 32000: validation loss = 2.3077, validation accuracy = 30.35%
Final test loss = 2.3093, test accuracy = 30.84%

Final 200K performance, with batch norm
Step 200000: validation loss = 2.2111, validation accuracy = 32.30%
Final test loss = 2.2184, test accuracy = 32.31%

