Baseline
49: train loss = 6.156622, time = 981ms, tok/s = 16701

Using TF32
49: train loss = 6.157476, time = 370ms, tok/s = 44269

On a faster machine - higher memory bandwidth
49: train loss = 6.157435, time = 334ms, tok/s = 48985

With bfloat16
49: train loss = 6.156698, time = 312ms, tok/s = 52594

With torch.compile
49: train loss = 6.177520, time = 157ms, tok/s = 104288

With flash attention
49: train loss = 6.215732, time = 121ms, tok/s = 135158

Using a nice number as vocab_size=50304
49: train loss = 6.215691, time = 121ms, tok/s = 135308

